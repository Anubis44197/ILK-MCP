
import os
import shutil
import base64
import json
import glob
import re
import datetime
import subprocess
import sys
from pathlib import Path
import hashlib

# --- GEREKLÄ° KÃœTÃœPHANELER ---
try:
    from bs4 import BeautifulSoup
    import pytesseract
    from pdf2image import convert_from_path
    from PIL import Image
    import pdfminer.high_level
except ImportError:
    print("âš ï¸  Eksik kÃ¼tÃ¼phaneler var! LÃ¼tfen: pip install -r requirements.txt")
    print("âš ï¸  OCR iÃ§in sisteminizde Tesseract ve Poppler kurulu olmalÄ±dÄ±r.")

# --- AYARLAR ---
USER_HOME = os.path.expanduser("~")
BASE_LIB_PATH = os.path.join(USER_HOME, "Desktop", "Esoteric_Library")
LIBRARY_ROOT = os.path.join(BASE_LIB_PATH, "Kutuphane") # TÃœRKÃ‡E KLASÃ–R
QUARANTINE_PATH = os.path.join(BASE_LIB_PATH, "Karantina") # TÃœRKÃ‡E KLASÃ–R

MANIFEST_PATH = os.path.join(BASE_LIB_PATH, "library_manifest.json")

# --- YARDIMCI FONSÄ°YONLAR ---
def ensure_dirs(path):
    if not os.path.exists(path): os.makedirs(path)

def load_manifest():
    if os.path.exists(MANIFEST_PATH):
        try:
            with open(MANIFEST_PATH, "r", encoding="utf-8") as f: return json.load(f)
        except: return {}
    return {}

def update_manifest(filename, status, dest_path=None, details=None):
    manifest = load_manifest()
    manifest[filename] = {
        "status": status,
        "processed_at": str(datetime.datetime.now()),
        "destination": dest_path,
        "details": details or {}
    }
    with open(MANIFEST_PATH, "w", encoding="utf-8") as f:
        json.dump(manifest, f, ensure_ascii=False, indent=4)

# --- KALÄ°TE KONTROL ---
# --- KALÄ°TE KONTROL (QUALITY INSPECTOR v2.0) ---
def check_quality(text):
    if not text: return False, "BoÅŸ Ä°Ã§erik"
    
    # 1. Uzunluk KontrolÃ¼ (Ã‡ok kÄ±sa metinler genellikle baÅŸlÄ±k veya sayfa nosudur)
    if len(text) < 100: return False, "Ã‡ok KÄ±sa (<100 karakter)"

    # Metni analiz iÃ§in hazÄ±rla
    text_no_space = text.replace(" ", "").replace("\n", "").strip()
    if len(text_no_space) == 0: return False, "Sadece BoÅŸluk/GÃ¶rÃ¼nmez Karakter"

    # 2. Temel GÃ¼rÃ¼ltÃ¼ OranÄ± (Sembol vs Harf)
    # Sadece harfleri ve sayÄ±larÄ± say
    clean_chars = re.sub(r'[^a-zA-Z0-9Ã¶Ã§ÅŸÄ±iÄŸÃ¼Ã–Ã‡ÅÄ°ÄÃœ]', '', text)
    ratio = len(clean_chars) / len(text_no_space)
    
    # EÄŸer karakterlerin %50'sinden fazlasÄ± sembolse (Ã¶rn: #, *, |, ----) reddet.
    if ratio < 0.5: 
        return False, f"YÃ¼ksek GÃ¼rÃ¼ltÃ¼ OranÄ± (%{int((1-ratio)*100)} Sembol)"

    # 3. Kelime Analizi (BoÅŸluksuz uzun yazÄ±lar veya tek harfler)
    words = text.split()
    if not words: return False, "Kelime BulunamadÄ±"
    
    avg_len = sum(len(w) for w in words) / len(words)
    
    # Ortalama kelime uzunluÄŸu Ã§ok fazlaysa (OCR boÅŸluklarÄ± kaÃ§Ä±rdÄ±ysa)
    if avg_len > 30: return False, "AnlamsÄ±z Uzun Bloklar (BoÅŸluk HatasÄ±)"
    # Ortalama kelime uzunluÄŸu Ã§ok azsa (t e k t e k h a r f l e r)
    if avg_len < 2: return False, "AÅŸÄ±rÄ± ParÃ§alÄ±/KÄ±sa Metin"

    # 4. Fonetik Kontrol (Sesli Harf OranÄ±)
    # TÃ¼rkÃ§e veya Ä°ngilizce anlamlÄ± bir metinde mutlaka sesli harf olmalÄ±.
    vowels = "aeiouÃ¶Ã¼Ä±iAEIOUÃ–ÃœÄ°I"
    vowel_count = sum(1 for c in text if c in vowels)
    vowel_ratio = vowel_count / len(text_no_space)
    
    if vowel_ratio < 0.15: # %15'ten az sesli harf varsa (Ã¶rn: "bcdfghjkl mnprst")
        return False, "Sesli Harf YetersizliÄŸi (Okunamaz Ä°Ã§erik)"

    return True, "OnaylandÄ±"

# --- OCR VE METÄ°N Ä°ÅLEME ---
def extract_text_from_pdf(pdf_path):
    text = ""
    print(f"      ğŸ‘€ PDF TaranÄ±yor (OCR)... {os.path.basename(pdf_path)}")
    
    try:
        text = pdfminer.high_level.extract_text(pdf_path)
        if text and len(text) > 100:
            return text
    except: pass

    try:
        images = convert_from_path(pdf_path)
        ocr_text = []
        for i, img in enumerate(images):
            if i % 5 == 0: print(f"         Sayfa {i+1}/{len(images)} iÅŸleniyor...")
            page_text = pytesseract.image_to_string(img, lang='eng+tur')
            ocr_text.append(page_text)
        return "\n".join(ocr_text)
    except Exception as e:
        print(f"         âŒ OCR HatasÄ±: {e}")
        return ""

def clean_text(text):
    lines = text.splitlines()
    cleaned = []
    headers_footers = ["back to top", "menu", "home", "contents", "index", "page", "bÃ¶lÃ¼m"]
    
    for line in lines:
        s = line.strip()
        if not s: continue
        if len(s) < 3: continue 
        if any(hf in s.lower() for hf in headers_footers) and len(s) < 20: continue
        cleaned.append(s)
    return "\n".join(cleaned)

def chunk_text(text, chunk_size=3000, overlap=500):
    chunks = []
    current_chunk = []
    current_len = 0
    
    paragraphs = text.split("\n\n")
    if len(paragraphs) < 2: paragraphs = text.split("\n") 

    for p in paragraphs:
        p = p.strip()
        if not p: continue
        
        if len(p) > chunk_size:
            sub_parts = re.split(r'(?<=[.!?])\s+', p)
            for sub in sub_parts:
                if current_len + len(sub) > chunk_size and current_chunk:
                    full_chunk = "\n\n".join(current_chunk)
                    chunks.append(full_chunk)
                    overlap_text = full_chunk[-overlap:] if len(full_chunk) > overlap else full_chunk
                    current_chunk = [overlap_text, sub]
                    current_len = len(overlap_text) + len(sub)
                else:
                    current_chunk.append(sub)
                    current_len += len(sub)
        else:
            if current_len + len(p) > chunk_size and current_chunk:
                full_chunk = "\n\n".join(current_chunk)
                chunks.append(full_chunk)
                overlap_text = full_chunk[-overlap:] if len(full_chunk) > overlap else full_chunk
                current_chunk = [overlap_text, p]
                current_len = len(overlap_text) + len(p)
            else:
                current_chunk.append(p)
                current_len += len(p)

    if current_chunk: chunks.append("\n\n".join(current_chunk))
    return chunks

# --- DOSYA Ä°ÅLEME MOTORU ---
def process_file_content(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    content = ""
    
    try:
        if ext == ".md" or ext == ".txt":
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        elif ext in [".html", ".htm"]:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                soup = BeautifulSoup(f.read(), "html.parser")
                for t in soup(["script", "style", "nav", "footer", "form"]): t.decompose()
                content = soup.get_text("\n")
        elif ext == ".pdf":
            content = extract_text_from_pdf(file_path)
        
        return clean_text(content), "OK"
    except Exception as e:
        return None, str(e)

def process_library():
    print("\nğŸ­ VERÄ° RAFÄ°NERÄ°SÄ° Ã‡ALIÅIYOR (YEÅÄ°L ENERJÄ° MODU)...")
    ensure_dirs(LIBRARY_ROOT)
    ensure_dirs(QUARANTINE_PATH)
    
    # Manifest'i yÃ¼kle
    manifest = load_manifest()
    
    if not os.path.exists(LIBRARY_ROOT):
        print("ğŸ“­ KÃ¼tÃ¼phane henÃ¼z boÅŸ.")
        return

    # KÃ¼tÃ¼phanedeki tÃ¼m dosyalarÄ± bul (Recursive)
    book_folders = [f.path for f in os.scandir(LIBRARY_ROOT) if f.is_dir()]
    
    processed_count = 0
    
    for book_folder in book_folders:
        folder_name = os.path.basename(book_folder)
        
        # Bu klasÃ¶rdeki iÅŸlenmemiÅŸ dosyalarÄ± bul
        # Okunabilir, Veri_Seti, assets hariÃ§ diÄŸerlerine bak
        files_to_process = []
        for root, dirs, files in os.walk(book_folder):
            if "Okunabilir" in root or "Veri_Seti" in root or "assets" in root:
                continue
            for f in files:
                # Desteklenen uzantÄ±lar
                if f.lower().endswith((".pdf", ".html", ".htm", ".md", ".txt")) and f != "README.md":
                     files_to_process.append(os.path.join(root, f))
        
        for file_path in files_to_process:
            filename = os.path.basename(file_path)
            
            # âš ï¸ KORUMA KALDIRIRILDI - Ä°stediÄŸin kadar indirebilirsin
            # if filename in manifest and manifest[filename].get("status") == "PROCESSED":
            #     continue

            print(f"\nâš™ï¸  Rafineri Ä°ÅŸliyor: {filename}")
            
            # 1. Ä°Ã‡ERÄ°ÄÄ° Ã‡IKAR
            content, status = process_file_content(file_path)
            
            # 2. KALÄ°TE KONTROL
            valid, msg = False, status
            if content:
                valid, msg = check_quality(content)
            
            if not valid:
                print(f"      â›” REDDEDÄ°LDÄ°: {msg}")
                # Karantinaya taÅŸÄ±
                try:
                    shutil.move(file_path, os.path.join(QUARANTINE_PATH, filename))
                except: pass
                # update_manifest(filename, "QUARANTINED", details={"reason": msg})  # KORUMA KALDIRIRILDI
                continue

            # 3. YAZMA VE DÃœZENLEME
            # Hedef: TÃ¼rkÃ§e KlasÃ¶rler
            human_dir = os.path.join(book_folder, "Okunabilir")
            machine_dir = os.path.join(book_folder, "Veri_Seti")
            
            ensure_dirs(human_dir)
            ensure_dirs(machine_dir)
            
            # Human Readable (Okunabilir)
            chunks = chunk_text(content, 3000, overlap=500)
            for i, chunk in enumerate(chunks):
                fname = f"{folder_name}_Bolum_{i+1:02d}.md"
                with open(os.path.join(human_dir, fname), "w", encoding="utf-8") as f:
                    f.write(f"# {folder_name} - BÃ¶lÃ¼m {i+1}\n\n{chunk}")
            
            # Machine Data (Veri_Seti)
            jsonl_path = os.path.join(machine_dir, "dataset.jsonl")
            with open(jsonl_path, "a", encoding="utf-8") as f:
                for i, chunk in enumerate(chunks):
                    content_hash = hashlib.md5(chunk.encode('utf-8')).hexdigest()
                    record = {
                        "id": f"{filename}_{i}_{content_hash[:8]}",
                        "source_id": filename,
                        "text": chunk,
                        "metadata": {
                            "title": folder_name, 
                            "part": i+1, 
                            "original_file": filename,
                            "content_hash": content_hash,
                            "timestamp": str(datetime.datetime.now())
                        }
                    }
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
            
            print(f"      âœ… Ä°ÅLENDÄ°: {len(chunks)} ParÃ§a.")
            
            # 4. ORÄ°JÄ°NAL DOSYAYI SAKLA (Hareket Ettirme, OlduÄŸu Yerde KalsÄ±n)
            # Manifest'e iÅŸlendi olarak iÅŸaretle, bÃ¶ylece bir daha iÅŸlemeyecek.
            # Ancak gÃ¶rsel dÃ¼zen iÃ§in isterseniz baÅŸÄ±na [Orijinal] ekleyebiliriz? 
            # KullanÄ±cÄ± "Direkt burada" dedi, olduÄŸu gibi bÄ±rakÄ±yoruz.
            
            # update_manifest(filename, "PROCESSED", dest_path=book_folder)  # KORUMA KALDIRIRILDI
            processed_count += 1

    if processed_count == 0:
        print("ğŸ’¤ Ä°ÅŸlenecek yeni dosya bulunamadÄ±.")
    else:
        print(f"\nâœ¨ {processed_count} adet yeni kaynak dÃ¼zenlendi ve arÅŸivlendi.")

if __name__ == "__main__":
    process_library()
